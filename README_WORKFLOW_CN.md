# MatterGen æ ‡å‡†å·¥ä½œæµç¨‹

<p align="center">
    <img src="assets/MatterGenlogo_.png" alt="MatterGen logo" width="400"/>
</p>

æœ¬æ–‡æ¡£æä¾› MatterGen çš„æ ‡å‡†ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹ï¼Œæ¶µç›–ä»ç¯å¢ƒé…ç½®åˆ°ç»“æœåˆ†æçš„å®Œæ•´ææ–™è®¾è®¡æµç¨‹ã€‚

## ğŸ“‹ ç›®å½•

- [ğŸš€ å¿«é€Ÿå¼€å§‹æµç¨‹](#-å¿«é€Ÿå¼€å§‹æµç¨‹)
- [ğŸ—ï¸ å®Œæ•´å·¥ä½œæµç¨‹](#-å®Œæ•´å·¥ä½œæµç¨‹)
- [ğŸ¯ åº”ç”¨åœºæ™¯](#-åº”ç”¨åœºæ™¯)
- [ğŸ“Š æ•°æ®ç®¡ç†](#-æ•°æ®ç®¡ç†)
- [ğŸ”„ å®éªŒç®¡ç†](#-å®éªŒç®¡ç†)
- [ğŸ“ˆ ç»“æœåˆ†æ](#-ç»“æœåˆ†æ)
- [ğŸ› ï¸ è‡ªåŠ¨åŒ–è„šæœ¬](#-è‡ªåŠ¨åŒ–è„šæœ¬)
- [ğŸ“š æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)

## ğŸš€ å¿«é€Ÿå¼€å§‹æµç¨‹

### æœ€å°å¯è¡Œå·¥ä½œæµç¨‹ (30åˆ†é’Ÿ)

é€‚åˆå¿«é€ŸéªŒè¯å’ŒåŸå‹å¼€å‘ï¼š

#### æ­¥éª¤ 1: ç¯å¢ƒå‡†å¤‡ (5åˆ†é’Ÿ)
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/microsoft/mattergen.git
cd mattergen

# åˆ›å»ºç¯å¢ƒ
pip install uv
uv venv .venv --python 3.10
source .venv/bin/activate
uv pip install -e .

# éªŒè¯å®‰è£…
python -c "import mattergen; print('âœ… å®‰è£…æˆåŠŸ')"
```

#### æ­¥éª¤ 2: æ•°æ®å‡†å¤‡ (10åˆ†é’Ÿ)
```bash
# ä¸‹è½½å°æ•°æ®é›† (MP-20)
git lfs pull -I data-release/mp-20/ --exclude=""
unzip data-release/mp-20/mp_20.zip -d datasets

# å¿«é€Ÿé¢„å¤„ç†
csv-to-dataset --csv-folder datasets/mp_20/ --dataset-name mp_20 --cache-folder datasets/cache
```

#### æ­¥éª¤ 3: å¿«é€Ÿç”Ÿæˆ (5åˆ†é’Ÿ)
```bash
# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆ
export MODEL_NAME=mattergen_base
export RESULTS_PATH=results/quick_test/

mattergen-generate $RESULTS_PATH \
  --pretrained-name=$MODEL_NAME \
  --batch_size=4 \
  --num_batches=1
```

#### æ­¥éª¤ 4: ç»“æœæŸ¥çœ‹ (5åˆ†é’Ÿ)
```bash
# æŸ¥çœ‹ç”Ÿæˆç»“æœ
ls $RESULTS_PATH
unzip $RESULTS_PATH/generated_crystals_cif.zip -d $RESULTS_PATH/cifs/
ls $RESULTS_PATH/cifs/  # æŸ¥çœ‹ç”Ÿæˆçš„ CIF æ–‡ä»¶

# å¿«é€Ÿç»Ÿè®¡
python -c "
import glob
cif_files = glob.glob('$RESULTS_PATH/cifs/*.cif')
print(f'ç”Ÿæˆäº† {len(cif_files)} ä¸ªæ™¶ä½“ç»“æ„')
"
```

#### æ­¥éª¤ 5: ç®€å•è¯„ä¼° (5åˆ†é’Ÿ)
```bash
# ä¸å¼›è±«çš„å¿«é€Ÿè¯„ä¼°
mattergen-evaluate \
  --structures_path=$RESULTS_PATH \
  --relax=False \
  --save_as="$RESULTS_PATH/quick_metrics.json"

# æŸ¥çœ‹è¯„ä¼°ç»“æœ
cat $RESULTS_PATH/quick_metrics.json | jq '.'
```

## ğŸ—ï¸ å®Œæ•´å·¥ä½œæµç¨‹

### ç«¯åˆ°ç«¯ææ–™è®¾è®¡æµç¨‹

#### ç¬¬ä¸€é˜¶æ®µ: é¡¹ç›®è§„åˆ’ (1-2å¤©)

##### 1.1 éœ€æ±‚åˆ†æ
```markdown
## é¡¹ç›®éœ€æ±‚æ¨¡æ¿

### ç›®æ ‡ææ–™ç±»å‹
- [ ] æ— æœºæ™¶ä½“ææ–™
- [ ] ç‰¹å®šåŒ–å­¦ä½“ç³»: ________________
- [ ] åŸå­æ•°é‡èŒƒå›´: ___ åˆ° ___ ä¸ªåŸå­

### ç›®æ ‡å±æ€§
- [ ] æ— æ¡ä»¶ç”Ÿæˆ (æ¢ç´¢æ–°ç»“æ„)
- [ ] ç£æ€§ææ–™ (ç£å¯†åº¦: ___ Î¼B/Ã…Â³)
- [ ] åŠå¯¼ä½“ææ–™ (å¸¦éš™: ___ eV)
- [ ] ç¡¬è´¨ææ–™ (ä½“ç§¯æ¨¡é‡: ___ GPa)
- [ ] ç¨³å®šææ–™ (å‡¸åŒ…ä¸Šæ–¹èƒ½é‡: < ___ eV/atom)
- [ ] å…¶ä»–: ________________

### ç”Ÿæˆæ•°é‡å’Œè´¨é‡è¦æ±‚
- ç›®æ ‡ç”Ÿæˆæ•°é‡: _______ ä¸ªç»“æ„
- è´¨é‡è¦æ±‚: æ–°é¢–æ€§ > ___%, ç¨³å®šæ€§ > ___%
- åç»­éªŒè¯æ–¹æ³•: DFTè®¡ç®— / å®éªŒåˆæˆ / å…¶ä»–

### è®¡ç®—èµ„æº
- å¯ç”¨GPU: _______ (å‹å·: _______)
- å†…å­˜é™åˆ¶: _______ GB
- æ—¶é—´é¢„ç®—: _______ å¤©
```

##### 1.2 æŠ€æœ¯è·¯çº¿é€‰æ‹©
```bash
# è¯„ä¼°ä¸åŒæ•°æ®é›†å’Œæ¨¡å‹çš„é€‚ç”¨æ€§
python scripts/evaluate_datasets.py --requirements requirements.yaml
```

#### ç¬¬äºŒé˜¶æ®µ: ç¯å¢ƒæ­å»º (åŠå¤©)

##### 2.1 å®Œæ•´ç¯å¢ƒé…ç½®
```bash
# å®Œæ•´å®‰è£…è„šæœ¬
#!/bin/bash
set -e

echo "ğŸš€ å¼€å§‹å®‰è£… MatterGen å®Œæ•´ç¯å¢ƒ..."

# 1. åŸºç¡€ç¯å¢ƒ
echo "ğŸ“¦ å®‰è£…åŸºç¡€ä¾èµ–..."
pip install uv git-lfs
git lfs install

# 2. é¡¹ç›®ç¯å¢ƒ
echo "ğŸ”§ åˆ›å»ºé¡¹ç›®ç¯å¢ƒ..."
uv venv .venv --python 3.10
source .venv/bin/activate

# 3. å®‰è£…åŒ…
echo "ğŸ“š å®‰è£… MatterGen..."
uv pip install -e .

# 4. éªŒè¯å®‰è£…
echo "âœ… éªŒè¯å®‰è£…..."
python -c "
import torch
import mattergen
print(f'âœ… PyTorch: {torch.__version__}')
print(f'âœ… CUDA: {torch.cuda.is_available()}')
print(f'âœ… MatterGen: å®‰è£…æˆåŠŸ')
"

# 5. æµ‹è¯•å·¥å…·
echo "ğŸ§ª æµ‹è¯•å‘½ä»¤è¡Œå·¥å…·..."
mattergen-train --help > /dev/null && echo "âœ… mattergen-train"
mattergen-generate --help > /dev/null && echo "âœ… mattergen-generate"
mattergen-evaluate --help > /dev/null && echo "âœ… mattergen-evaluate"

echo "ğŸ‰ å®‰è£…å®Œæˆï¼"
```

##### 2.2 æ•°æ®é›†å‡†å¤‡
```bash
# æ•°æ®é›†é€‰æ‹©å’Œå‡†å¤‡è„šæœ¬
#!/bin/bash

echo "ğŸ“Š å‡†å¤‡æ•°æ®é›†..."

# æ ¹æ®é¡¹ç›®éœ€æ±‚é€‰æ‹©æ•°æ®é›†
if [ "$DATASET_SIZE" = "small" ]; then
    echo "ğŸ“¦ ä¸‹è½½ MP-20 æ•°æ®é›† (çº¦45kç»“æ„)..."
    git lfs pull -I data-release/mp-20/ --exclude=""
    unzip data-release/mp-20/mp_20.zip -d datasets
    DATASET_NAME="mp_20"
elif [ "$DATASET_SIZE" = "large" ]; then
    echo "ğŸ“¦ ä¸‹è½½ Alex-MP-20 æ•°æ®é›† (çº¦600kç»“æ„)..."
    git lfs pull -I data-release/alex-mp/alex_mp_20.zip --exclude=""
    unzip data-release/alex-mp/alex_mp_20.zip -d datasets
    DATASET_NAME="alex_mp_20"
fi

# æ•°æ®é¢„å¤„ç†
echo "âš™ï¸ é¢„å¤„ç†æ•°æ®é›†..."
csv-to-dataset \
  --csv-folder datasets/$DATASET_NAME/ \
  --dataset-name $DATASET_NAME \
  --cache-folder datasets/cache

# æ•°æ®éªŒè¯
echo "âœ… éªŒè¯æ•°æ®å®Œæ•´æ€§..."
python scripts/validate_dataset.py --dataset $DATASET_NAME

echo "ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼"
```

#### ç¬¬ä¸‰é˜¶æ®µ: æ¨¡å‹å‡†å¤‡ (1-3å¤©)

##### 3.1 åŸºç¡€æ¨¡å‹éªŒè¯
```bash
# éªŒè¯é¢„è®­ç»ƒæ¨¡å‹
echo "ğŸ§ª æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹..."

export MODEL_NAME=mattergen_base
export TEST_RESULTS=results/model_test/

# å°è§„æ¨¡æµ‹è¯•ç”Ÿæˆ
mattergen-generate $TEST_RESULTS \
  --pretrained-name=$MODEL_NAME \
  --batch_size=4 \
  --num_batches=1

# æ£€æŸ¥ç”Ÿæˆè´¨é‡
mattergen-evaluate \
  --structures_path=$TEST_RESULTS \
  --relax=False \
  --save_as="$TEST_RESULTS/test_metrics.json"

echo "âœ… åŸºç¡€æ¨¡å‹éªŒè¯å®Œæˆ"
```

##### 3.2 æ¨¡å‹å¾®è°ƒ (å¯é€‰)
```bash
# æ ¹æ®é¡¹ç›®éœ€æ±‚è¿›è¡Œå¾®è°ƒ
if [ "$NEED_FINETUNING" = "true" ]; then
    echo "ğŸ¯ å¼€å§‹æ¨¡å‹å¾®è°ƒ..."
    
    export PROPERTY=$TARGET_PROPERTY
    export FINETUNE_OUTPUT="outputs/finetune_${PROPERTY}"
    
    # å¾®è°ƒè®­ç»ƒ
    mattergen-finetune \
      adapter.pretrained_name=mattergen_base \
      data_module=alex_mp_20 \
      +lightning_module/diffusion_module/model/property_embeddings@adapter.adapter.property_embeddings_adapt.$PROPERTY=$PROPERTY \
      ~trainer.logger \
      data_module.properties=["$PROPERTY"] \
      trainer.max_epochs=100
    
    # éªŒè¯å¾®è°ƒæ•ˆæœ
    LATEST_RUN=$(ls -t outputs/singlerun/ | head -1)
    export FINETUNED_MODEL="outputs/singlerun/$LATEST_RUN/checkpoints/last.ckpt"
    
    echo "âœ… æ¨¡å‹å¾®è°ƒå®Œæˆ"
fi
```

#### ç¬¬å››é˜¶æ®µ: å¤§è§„æ¨¡ç”Ÿæˆ (1-2å¤©)

##### 4.1 ç”Ÿæˆå‚æ•°ä¼˜åŒ–
```bash
# æ‰¹é‡æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ
echo "ğŸ”§ ä¼˜åŒ–ç”Ÿæˆå‚æ•°..."

# æµ‹è¯•ä¸åŒå¼•å¯¼å¼ºåº¦
for guidance in 0.0 1.0 2.0 3.0 5.0; do
    echo "æµ‹è¯•å¼•å¯¼å¼ºåº¦: $guidance"
    
    mattergen-generate results/param_test/guidance_$guidance \
      --pretrained-name=$MODEL_NAME \
      --batch_size=16 \
      --num_batches=2 \
      --properties_to_condition_on="{'$TARGET_PROPERTY': $TARGET_VALUE}" \
      --diffusion_guidance_factor=$guidance
    
    # å¿«é€Ÿè¯„ä¼°
    mattergen-evaluate \
      --structures_path=results/param_test/guidance_$guidance \
      --relax=False \
      --save_as="results/param_test/guidance_$guidance/metrics.json"
done

# åˆ†ææœ€ä¼˜å‚æ•°
python scripts/analyze_parameters.py --results_dir results/param_test/
```

##### 4.2 å¤§è§„æ¨¡ç”Ÿæˆ
```bash
# å¤§è§„æ¨¡ç”Ÿæˆè„šæœ¬
#!/bin/bash

echo "ğŸš€ å¼€å§‹å¤§è§„æ¨¡ç”Ÿæˆ..."

export MODEL_NAME=${FINAL_MODEL_NAME}
export RESULTS_PATH=results/production/
export TOTAL_STRUCTURES=${TARGET_STRUCTURE_COUNT}
export BATCH_SIZE=${OPTIMIZED_BATCH_SIZE}
export GUIDANCE_FACTOR=${OPTIMIZED_GUIDANCE}

# è®¡ç®—éœ€è¦çš„æ‰¹æ¬¡æ•°
NUM_BATCHES=$((TOTAL_STRUCTURES / BATCH_SIZE))

echo "ğŸ“Š ç”Ÿæˆå‚æ•°:"
echo "  - æ¨¡å‹: $MODEL_NAME"
echo "  - æ€»ç»“æ„æ•°: $TOTAL_STRUCTURES"
echo "  - æ‰¹æ¬¡å¤§å°: $BATCH_SIZE"
echo "  - æ‰¹æ¬¡æ•°é‡: $NUM_BATCHES"
echo "  - å¼•å¯¼å¼ºåº¦: $GUIDANCE_FACTOR"

# åˆ†æ‰¹ç”Ÿæˆï¼Œé¿å…å•æ¬¡è¿è¡Œè¿‡é•¿
BATCH_PER_RUN=10
RUNS=$((NUM_BATCHES / BATCH_PER_RUN))

for run in $(seq 1 $RUNS); do
    echo "ğŸ”„ æ‰§è¡Œç”Ÿæˆè½®æ¬¡ $run/$RUNS..."
    
    OUTPUT_DIR="${RESULTS_PATH}/run_${run}"
    
    mattergen-generate $OUTPUT_DIR \
      --pretrained-name=$MODEL_NAME \
      --batch_size=$BATCH_SIZE \
      --num_batches=$BATCH_PER_RUN \
      --properties_to_condition_on="{'$TARGET_PROPERTY': $TARGET_VALUE}" \
      --diffusion_guidance_factor=$GUIDANCE_FACTOR
    
    echo "âœ… è½®æ¬¡ $run å®Œæˆ"
done

echo "ğŸ‰ å¤§è§„æ¨¡ç”Ÿæˆå®Œæˆï¼"
```

#### ç¬¬äº”é˜¶æ®µ: è´¨é‡è¯„ä¼° (1-2å¤©)

##### 5.1 ç»“æ„è´¨é‡è¯„ä¼°
```bash
# å…¨é¢è´¨é‡è¯„ä¼°
echo "ğŸ“ˆ å¼€å§‹è´¨é‡è¯„ä¼°..."

# åˆå¹¶æ‰€æœ‰ç”Ÿæˆç»“æœ
python scripts/merge_results.py \
  --input_dirs results/production/run_* \
  --output_dir results/production/merged/

# æ‰§è¡Œå®Œæ•´è¯„ä¼°
echo "ğŸ”¬ æ‰§è¡Œå®Œæ•´ç»“æ„è¯„ä¼°..."
git lfs pull -I data-release/alex-mp/reference_MP2020correction.gz --exclude=""

mattergen-evaluate \
  --structures_path=results/production/merged/ \
  --relax=True \
  --structure_matcher='disordered' \
  --save_as="results/production/merged/full_metrics.json" \
  --structures_output_path="results/production/merged/relaxed_structures.extxyz"

echo "âœ… ç»“æ„è¯„ä¼°å®Œæˆ"
```

##### 5.2 å±æ€§åˆ†æ
```bash
# å±æ€§ç»Ÿè®¡å’Œåˆ†æ
echo "ğŸ“Š åˆ†æç”Ÿæˆç»“æ„çš„å±æ€§åˆ†å¸ƒ..."

python scripts/property_analysis.py \
  --structures_path results/production/merged/ \
  --target_property $TARGET_PROPERTY \
  --target_value $TARGET_VALUE \
  --output_dir results/production/analysis/

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
python scripts/generate_report.py \
  --metrics_file results/production/merged/full_metrics.json \
  --analysis_dir results/production/analysis/ \
  --output_file results/production/final_report.html
```

#### ç¬¬å…­é˜¶æ®µ: ç»“æœç­›é€‰ (åŠå¤©)

##### 6.1 å¤šæ¡ä»¶ç­›é€‰
```bash
# é«˜è´¨é‡å€™é€‰ç»“æ„ç­›é€‰
echo "ğŸ¯ ç­›é€‰é«˜è´¨é‡å€™é€‰ç»“æ„..."

python scripts/filter_candidates.py \
  --structures_path results/production/merged/ \
  --metrics_file results/production/merged/full_metrics.json \
  --criteria "
    novelty > 0.95 and
    stability > 0.8 and
    validity == 1.0 and
    target_property_error < 0.1
  " \
  --output_dir results/production/candidates/ \
  --max_candidates 100

echo "âœ… ç­›é€‰å‡º $(ls results/production/candidates/*.cif | wc -l) ä¸ªå€™é€‰ç»“æ„"
```

##### 6.2 å€™é€‰ç»“æ„æ’åº
```bash
# æ ¹æ®ç»¼åˆè¯„åˆ†æ’åº
python scripts/rank_candidates.py \
  --candidates_dir results/production/candidates/ \
  --ranking_strategy "weighted_score" \
  --weights "novelty:0.3,stability:0.4,target_property:0.3" \
  --output_file results/production/ranked_candidates.csv

# è¾“å‡ºå‰10åå€™é€‰
head -11 results/production/ranked_candidates.csv
```

#### ç¬¬ä¸ƒé˜¶æ®µ: éªŒè¯å‡†å¤‡ (åŠå¤©)

##### 7.1 DFTè®¡ç®—å‡†å¤‡
```bash
# ä¸ºDFTè®¡ç®—å‡†å¤‡è¾“å…¥æ–‡ä»¶
echo "âš™ï¸ å‡†å¤‡DFTè®¡ç®—è¾“å…¥..."

python scripts/prepare_dft_inputs.py \
  --candidates_file results/production/ranked_candidates.csv \
  --top_n 20 \
  --dft_software VASP \
  --output_dir results/production/dft_inputs/

echo "âœ… DFTè¾“å…¥æ–‡ä»¶å‡†å¤‡å®Œæˆ"
```

##### 7.2 å®éªŒéªŒè¯ä¿¡æ¯
```bash
# ç”Ÿæˆå®éªŒéªŒè¯æŒ‡å—
python scripts/generate_synthesis_guide.py \
  --candidates_file results/production/ranked_candidates.csv \
  --top_n 10 \
  --output_file results/production/synthesis_guide.md

echo "ğŸ“‹ å®éªŒåˆæˆæŒ‡å—å·²ç”Ÿæˆ"
```

## ğŸ¯ åº”ç”¨åœºæ™¯

### åœºæ™¯1: ç£æ€§ææ–™è®¾è®¡

#### ç›®æ ‡è®¾å®š
```bash
# è®¾è®¡é«˜ç£çŸ©å¯†åº¦ææ–™
export TARGET_PROPERTY="dft_mag_density"
export TARGET_VALUE="1.5"  # Î¼B/Ã…Â³
export CHEMICAL_CONSTRAINTS="Fe,Co,Ni"  # é™åˆ¶åœ¨é“ç£æ€§å…ƒç´ 
```

#### ä¸“ç”¨å·¥ä½œæµç¨‹
```bash
# 1. æ•°æ®ç­›é€‰
python scripts/filter_training_data.py \
  --property $TARGET_PROPERTY \
  --min_value 1.0 \
  --chemical_elements $CHEMICAL_CONSTRAINTS \
  --output_dir datasets/magnetic_focused/

# 2. ä¸“ç”¨å¾®è°ƒ
mattergen-finetune \
  adapter.pretrained_name=mattergen_base \
  data_module.data_path=datasets/magnetic_focused/ \
  +lightning_module/diffusion_module/model/property_embeddings@adapter.adapter.property_embeddings_adapt.$TARGET_PROPERTY=$TARGET_PROPERTY \
  data_module.properties=["$TARGET_PROPERTY"] \
  trainer.max_epochs=200

# 3. ç£æ€§ææ–™ç”Ÿæˆ
mattergen-generate results/magnetic_materials/ \
  --model_path=$FINETUNED_MODEL \
  --batch_size=32 \
  --num_batches=50 \
  --properties_to_condition_on="{'$TARGET_PROPERTY': $TARGET_VALUE}" \
  --diffusion_guidance_factor=3.0

# 4. ç£æ€§èƒ½è¯„ä¼°
python scripts/magnetic_analysis.py \
  --structures_path results/magnetic_materials/ \
  --output_dir results/magnetic_analysis/
```

### åœºæ™¯2: è¶…ç¡¬ææ–™å‘ç°

#### ç›®æ ‡è®¾å®š
```bash
# è®¾è®¡é«˜ä½“ç§¯æ¨¡é‡ææ–™
export TARGET_PROPERTY="ml_bulk_modulus"
export TARGET_VALUE="400"  # GPa
export HARDNESS_THRESHOLD="10"  # GPa (Vickersç¡¬åº¦)
```

#### ä¸“ç”¨å·¥ä½œæµç¨‹
```bash
# 1. å¤šå±æ€§ä¼˜åŒ–
mattergen-generate results/superhard_materials/ \
  --pretrained-name=ml_bulk_modulus \
  --batch_size=32 \
  --num_batches=100 \
  --properties_to_condition_on="{'$TARGET_PROPERTY': $TARGET_VALUE}" \
  --diffusion_guidance_factor=2.5

# 2. ç¡¬åº¦é¢„æµ‹å’Œç­›é€‰
python scripts/predict_hardness.py \
  --structures_path results/superhard_materials/ \
  --model_path models/hardness_predictor.pkl \
  --threshold $HARDNESS_THRESHOLD \
  --output_dir results/superhard_candidates/

# 3. åŠ›å­¦æ€§èƒ½åˆ†æ
python scripts/mechanical_analysis.py \
  --candidates_dir results/superhard_candidates/ \
  --analysis_type "elastic_constants" \
  --output_dir results/mechanical_analysis/
```

### åœºæ™¯3: ç¨³å®šæ–°åŒ–åˆç‰©è®¾è®¡

#### ç›®æ ‡è®¾å®š
```bash
# è®¾è®¡ä½å‡¸åŒ…ä¸Šæ–¹èƒ½é‡çš„æ–°åŒ–åˆç‰©
export TARGET_PROPERTY="energy_above_hull"
export TARGET_VALUE="0.02"  # eV/atom
export NOVELTY_THRESHOLD="0.98"
```

#### ä¸“ç”¨å·¥ä½œæµç¨‹
```bash
# 1. ç¨³å®šæ€§çº¦æŸç”Ÿæˆ
mattergen-generate results/stable_compounds/ \
  --pretrained-name=chemical_system_energy_above_hull \
  --batch_size=64 \
  --num_batches=200 \
  --properties_to_condition_on="{'$TARGET_PROPERTY': $TARGET_VALUE}" \
  --diffusion_guidance_factor=4.0

# 2. æ–°é¢–æ€§ç­›é€‰
python scripts/novelty_filter.py \
  --structures_path results/stable_compounds/ \
  --reference_databases "MP,OQMD,AFLOW" \
  --novelty_threshold $NOVELTY_THRESHOLD \
  --output_dir results/novel_stable_compounds/

# 3. çƒ­åŠ›å­¦ç¨³å®šæ€§éªŒè¯
python scripts/stability_analysis.py \
  --candidates_dir results/novel_stable_compounds/ \
  --analysis_methods "convex_hull,phase_diagram" \
  --output_dir results/stability_analysis/
```

## ğŸ“Š æ•°æ®ç®¡ç†

### æ•°æ®ç»„ç»‡ç»“æ„

#### æ¨èç›®å½•ç»“æ„
```
project_materials_design/
â”œâ”€â”€ data/                          # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ mp_20/                # MP-20 æ•°æ®é›†
â”‚   â”‚   â”œâ”€â”€ alex_mp_20/           # Alex-MP-20 æ•°æ®é›†
â”‚   â”‚   â””â”€â”€ custom/               # è‡ªå®šä¹‰æ•°æ®é›†
â”‚   â””â”€â”€ cache/                    # é¢„å¤„ç†ç¼“å­˜
â”œâ”€â”€ models/                       # æ¨¡å‹ç›¸å…³
â”‚   â”œâ”€â”€ pretrained/               # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ finetuned/               # å¾®è°ƒæ¨¡å‹
â”‚   â””â”€â”€ checkpoints/             # è®­ç»ƒæ£€æŸ¥ç‚¹
â”œâ”€â”€ experiments/                  # å®éªŒè®°å½•
â”‚   â”œâ”€â”€ exp_001_baseline/
â”‚   â”œâ”€â”€ exp_002_magnetic/
â”‚   â””â”€â”€ exp_003_superhard/
â”œâ”€â”€ results/                      # ç”Ÿæˆç»“æœ
â”‚   â”œâ”€â”€ structures/               # ç”Ÿæˆçš„ç»“æ„
â”‚   â”œâ”€â”€ analysis/                # åˆ†æç»“æœ
â”‚   â””â”€â”€ reports/                 # æŠ¥å‘Šæ–‡æ¡£
â”œâ”€â”€ scripts/                     # è‡ªåŠ¨åŒ–è„šæœ¬
â”œâ”€â”€ configs/                     # é…ç½®æ–‡ä»¶
â””â”€â”€ docs/                        # é¡¹ç›®æ–‡æ¡£
```

#### æ•°æ®ç‰ˆæœ¬æ§åˆ¶
```bash
# ä½¿ç”¨ DVC è¿›è¡Œæ•°æ®ç‰ˆæœ¬æ§åˆ¶
pip install dvc[gdrive]  # æˆ–å…¶ä»–äº‘å­˜å‚¨

# åˆå§‹åŒ–DVC
dvc init

# æ·»åŠ æ•°æ®åˆ°ç‰ˆæœ¬æ§åˆ¶
dvc add data/datasets/
dvc add results/

# æäº¤åˆ°Git
git add data/datasets/.dvc results/.dvc .dvcignore
git commit -m "æ·»åŠ æ•°æ®ç‰ˆæœ¬æ§åˆ¶"

# æ¨é€æ•°æ®åˆ°è¿œç¨‹å­˜å‚¨
dvc remote add -d gdrive gdrive://your-gdrive-folder-id
dvc push
```

### å®éªŒè®°å½•æ¨¡æ¿

#### å®éªŒé…ç½®æ–‡ä»¶
```yaml
# experiments/exp_001_baseline/config.yaml
experiment:
  name: "baseline_unconditional_generation"
  description: "åŸºç¡€æ— æ¡ä»¶ç”Ÿæˆå®éªŒ"
  date: "2025-01-XX"
  researcher: "ç ”ç©¶å‘˜å§“å"

model:
  name: "mattergen_base"
  type: "pretrained"
  
generation:
  batch_size: 32
  num_batches: 100
  total_structures: 3200
  
evaluation:
  relax: true
  structure_matcher: "disordered"
  
goals:
  - "éªŒè¯åŸºç¡€ç”Ÿæˆè´¨é‡"
  - "å»ºç«‹è´¨é‡åŸºçº¿"
  - "æµ‹è¯•è®¡ç®—èµ„æºéœ€æ±‚"

expected_results:
  novelty: "> 0.90"
  validity: "> 0.95"
  uniqueness: "> 0.85"
```

#### å®éªŒæ‰§è¡Œè„šæœ¬
```bash
#!/bin/bash
# experiments/exp_001_baseline/run_experiment.sh

# å®éªŒç¯å¢ƒè®¾ç½®
export EXP_NAME="exp_001_baseline"
export EXP_DIR="experiments/$EXP_NAME"
export RESULTS_DIR="$EXP_DIR/results"

# è®°å½•å®éªŒå¼€å§‹
echo "ğŸš€ å¼€å§‹å®éªŒ: $EXP_NAME" | tee $EXP_DIR/experiment.log
echo "å¼€å§‹æ—¶é—´: $(date)" | tee -a $EXP_DIR/experiment.log

# ç”Ÿæˆç»“æ„
mattergen-generate $RESULTS_DIR \
  --pretrained-name=mattergen_base \
  --batch_size=32 \
  --num_batches=100 2>&1 | tee -a $EXP_DIR/experiment.log

# è¯„ä¼°ç»“æœ
mattergen-evaluate \
  --structures_path=$RESULTS_DIR \
  --relax=True \
  --save_as="$RESULTS_DIR/metrics.json" 2>&1 | tee -a $EXP_DIR/experiment.log

# ç”ŸæˆæŠ¥å‘Š
python scripts/generate_experiment_report.py \
  --config $EXP_DIR/config.yaml \
  --results $RESULTS_DIR/metrics.json \
  --output $EXP_DIR/report.html

echo "âœ… å®éªŒå®Œæˆ: $(date)" | tee -a $EXP_DIR/experiment.log
```

## ğŸ”„ å®éªŒç®¡ç†

### å®éªŒè·Ÿè¸ªç³»ç»Ÿ

#### ä½¿ç”¨ MLflow è·Ÿè¸ªå®éªŒ
```python
# scripts/experiment_tracker.py
import mlflow
import mlflow.pytorch
from pathlib import Path

class ExperimentTracker:
    def __init__(self, experiment_name: str):
        mlflow.set_experiment(experiment_name)
        self.run = mlflow.start_run()
    
    def log_config(self, config: dict):
        """è®°å½•å®éªŒé…ç½®"""
        for key, value in config.items():
            mlflow.log_param(key, value)
    
    def log_metrics(self, metrics: dict):
        """è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        for key, value in metrics.items():
            mlflow.log_metric(key, value)
    
    def log_model(self, model_path: str):
        """è®°å½•æ¨¡å‹æ–‡ä»¶"""
        mlflow.log_artifact(model_path, "model")
    
    def log_results(self, results_dir: str):
        """è®°å½•ç”Ÿæˆç»“æœ"""
        mlflow.log_artifacts(results_dir, "results")
    
    def finish(self):
        """ç»“æŸå®éªŒè®°å½•"""
        mlflow.end_run()

# ä½¿ç”¨ç¤ºä¾‹
tracker = ExperimentTracker("magnetic_materials_design")
tracker.log_config({
    "model": "dft_mag_density",
    "target_value": 1.5,
    "guidance_factor": 3.0
})
# ... å®éªŒæ‰§è¡Œ ...
tracker.log_metrics(evaluation_results)
tracker.finish()
```

#### è‡ªåŠ¨åŒ–å®éªŒæ¯”è¾ƒ
```python
# scripts/compare_experiments.py
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

def compare_experiments(experiment_dirs: list):
    """æ¯”è¾ƒå¤šä¸ªå®éªŒçš„ç»“æœ"""
    
    results = []
    
    for exp_dir in experiment_dirs:
        # è¯»å–å®éªŒé…ç½®å’Œç»“æœ
        config_file = Path(exp_dir) / "config.yaml"
        metrics_file = Path(exp_dir) / "results" / "metrics.json"
        
        if config_file.exists() and metrics_file.exists():
            config = yaml.load(config_file.read_text())
            metrics = json.load(metrics_file.open())
            
            result = {
                "experiment": config["experiment"]["name"],
                "model": config["model"]["name"],
                **metrics
            }
            results.append(result)
    
    # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
    df = pd.DataFrame(results)
    
    # ç”Ÿæˆæ¯”è¾ƒå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    df.plot(x="experiment", y="novelty", kind="bar", ax=axes[0,0])
    df.plot(x="experiment", y="validity", kind="bar", ax=axes[0,1])
    df.plot(x="experiment", y="stability", kind="bar", ax=axes[1,0])
    df.plot(x="experiment", y="uniqueness", kind="bar", ax=axes[1,1])
    
    plt.tight_layout()
    plt.savefig("experiments/comparison.png")
    
    return df
```

## ğŸ“ˆ ç»“æœåˆ†æ

### ç”Ÿæˆè´¨é‡åˆ†æ

#### ç»¼åˆè´¨é‡è¯„ä¼°è„šæœ¬
```python
# scripts/quality_analysis.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ase.io import read
import json

class QualityAnalyzer:
    def __init__(self, results_dir: str):
        self.results_dir = Path(results_dir)
        self.structures = self.load_structures()
        self.metrics = self.load_metrics()
    
    def load_structures(self):
        """åŠ è½½ç”Ÿæˆçš„ç»“æ„"""
        extxyz_file = self.results_dir / "generated_crystals.extxyz"
        if extxyz_file.exists():
            return read(str(extxyz_file), ":")
        return []
    
    def load_metrics(self):
        """åŠ è½½è¯„ä¼°æŒ‡æ ‡"""
        metrics_file = self.results_dir / "metrics.json"
        if metrics_file.exists():
            return json.load(metrics_file.open())
        return {}
    
    def analyze_composition_diversity(self):
        """åˆ†æç»„åˆ†å¤šæ ·æ€§"""
        compositions = []
        for structure in self.structures:
            symbols = structure.get_chemical_symbols()
            composition = "-".join(sorted(set(symbols)))
            compositions.append(composition)
        
        composition_counts = pd.Series(compositions).value_counts()
        
        plt.figure(figsize=(12, 6))
        composition_counts.head(20).plot(kind='bar')
        plt.title('Top 20 Chemical Compositions')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(self.results_dir / "composition_diversity.png")
        
        return composition_counts
    
    def analyze_structure_properties(self):
        """åˆ†æç»“æ„å±æ€§åˆ†å¸ƒ"""
        properties = {
            'num_atoms': [],
            'density': [],
            'volume': []
        }
        
        for structure in self.structures:
            properties['num_atoms'].append(len(structure))
            properties['density'].append(structure.get_density())
            properties['volume'].append(structure.get_volume())
        
        # ç»˜åˆ¶åˆ†å¸ƒå›¾
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        for i, (prop, values) in enumerate(properties.items()):
            axes[i].hist(values, bins=30, alpha=0.7)
            axes[i].set_title(f'{prop.replace("_", " ").title()} Distribution')
            axes[i].set_ylabel('Count')
        
        plt.tight_layout()
        plt.savefig(self.results_dir / "structure_properties.png")
        
        return properties
    
    def generate_quality_report(self):
        """ç”Ÿæˆè´¨é‡åˆ†ææŠ¥å‘Š"""
        report = {
            'total_structures': len(self.structures),
            'metrics_summary': self.metrics,
            'composition_diversity': self.analyze_composition_diversity().to_dict(),
            'structure_properties': self.analyze_structure_properties()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(self.results_dir / "quality_report.json", 'w') as f:
            json.dump(report, f, indent=2)
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
analyzer = QualityAnalyzer("results/production/merged/")
report = analyzer.generate_quality_report()
```

### å±æ€§-ç»“æ„å…³ç³»åˆ†æ

#### å±æ€§å…³è”åˆ†æ
```python
# scripts/property_structure_analysis.py
class PropertyStructureAnalyzer:
    def __init__(self, structures_file: str, properties_file: str):
        self.structures = read(structures_file, ":")
        self.properties = pd.read_csv(properties_file)
    
    def extract_structural_features(self):
        """æå–ç»“æ„ç‰¹å¾"""
        features = []
        
        for structure in self.structures:
            feature = {
                'num_atoms': len(structure),
                'density': structure.get_density(),
                'volume': structure.get_volume(),
                'avg_coordination': self.calc_avg_coordination(structure),
                'space_group': self.get_space_group(structure),
                'composition_complexity': len(set(structure.get_chemical_symbols()))
            }
            features.append(feature)
        
        return pd.DataFrame(features)
    
    def analyze_property_correlations(self, target_property: str):
        """åˆ†æå±æ€§ä¸ç»“æ„ç‰¹å¾çš„å…³è”"""
        structural_features = self.extract_structural_features()
        
        # åˆå¹¶ç»“æ„ç‰¹å¾å’Œå±æ€§æ•°æ®
        data = pd.concat([structural_features, self.properties[target_property]], axis=1)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlations = data.corr()[target_property].sort_values(ascending=False)
        
        # ç»˜åˆ¶ç›¸å…³æ€§çƒ­å›¾
        plt.figure(figsize=(10, 8))
        sns.heatmap(data.corr(), annot=True, cmap='coolwarm', center=0)
        plt.title(f'Property-Structure Correlations for {target_property}')
        plt.tight_layout()
        plt.savefig(f"analysis/{target_property}_correlations.png")
        
        return correlations
    
    def identify_design_rules(self, target_property: str, threshold: float):
        """è¯†åˆ«è®¾è®¡è§„åˆ™"""
        data = self.extract_structural_features()
        data[target_property] = self.properties[target_property]
        
        # ç­›é€‰é«˜æ€§èƒ½æ ·æœ¬
        high_performance = data[data[target_property] > threshold]
        
        # åˆ†æé«˜æ€§èƒ½æ ·æœ¬çš„å…±åŒç‰¹å¾
        rules = {}
        for feature in data.columns:
            if feature != target_property:
                rules[feature] = {
                    'mean': high_performance[feature].mean(),
                    'std': high_performance[feature].std(),
                    'range': (high_performance[feature].min(), high_performance[feature].max())
                }
        
        return rules
```

## ğŸ› ï¸ è‡ªåŠ¨åŒ–è„šæœ¬

### ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–è„šæœ¬

#### ä¸»æ§è„šæœ¬
```python
#!/usr/bin/env python3
# scripts/automated_workflow.py

import argparse
import yaml
import subprocess
import logging
from pathlib import Path
from datetime import datetime

class AutomatedWorkflow:
    def __init__(self, config_file: str):
        self.config = yaml.load(Path(config_file).read_text())
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = f"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def run_stage(self, stage_name: str, commands: list):
        """æ‰§è¡Œå·¥ä½œæµç¨‹é˜¶æ®µ"""
        self.logger.info(f"å¼€å§‹æ‰§è¡Œé˜¶æ®µ: {stage_name}")
        
        for i, command in enumerate(commands):
            self.logger.info(f"æ‰§è¡Œå‘½ä»¤ {i+1}/{len(commands)}: {command}")
            
            try:
                result = subprocess.run(
                    command, 
                    shell=True, 
                    capture_output=True, 
                    text=True,
                    check=True
                )
                self.logger.info(f"å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            except subprocess.CalledProcessError as e:
                self.logger.error(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
                self.logger.error(f"é”™è¯¯è¾“å‡º: {e.stderr}")
                raise
        
        self.logger.info(f"é˜¶æ®µ {stage_name} å®Œæˆ")
    
    def run_full_workflow(self):
        """æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹"""
        workflow_config = self.config['workflow']
        
        for stage in workflow_config['stages']:
            stage_name = stage['name']
            
            if stage.get('enabled', True):
                commands = stage['commands']
                self.run_stage(stage_name, commands)
            else:
                self.logger.info(f"è·³è¿‡é˜¶æ®µ: {stage_name} (å·²ç¦ç”¨)")
        
        self.logger.info("å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(description='MatterGen è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹')
    parser.add_argument('--config', required=True, help='å·¥ä½œæµç¨‹é…ç½®æ–‡ä»¶')
    args = parser.parse_args()
    
    workflow = AutomatedWorkflow(args.config)
    workflow.run_full_workflow()

if __name__ == "__main__":
    main()
```

#### å·¥ä½œæµç¨‹é…ç½®æ¨¡æ¿
```yaml
# configs/workflow_template.yaml
workflow:
  name: "automated_materials_design"
  description: "è‡ªåŠ¨åŒ–ææ–™è®¾è®¡å·¥ä½œæµç¨‹"
  
  variables:
    MODEL_NAME: "mattergen_base"
    TARGET_PROPERTY: "dft_mag_density"
    TARGET_VALUE: "1.5"
    BATCH_SIZE: "32"
    NUM_BATCHES: "50"
    RESULTS_PATH: "results/automated_run"
  
  stages:
    - name: "environment_setup"
      enabled: true
      commands:
        - "source .venv/bin/activate"
        - "python -c 'import mattergen; print(\"âœ… Environment ready\")'"
    
    - name: "data_preparation"
      enabled: true
      commands:
        - "git lfs pull -I data-release/alex-mp/alex_mp_20.zip --exclude=''"
        - "unzip -o data-release/alex-mp/alex_mp_20.zip -d datasets"
        - "csv-to-dataset --csv-folder datasets/alex_mp_20/ --dataset-name alex_mp_20 --cache-folder datasets/cache"
    
    - name: "model_finetuning"
      enabled: false  # å¯é€‰é˜¶æ®µ
      commands:
        - "mattergen-finetune adapter.pretrained_name=${MODEL_NAME} data_module=alex_mp_20 +lightning_module/diffusion_module/model/property_embeddings@adapter.adapter.property_embeddings_adapt.${TARGET_PROPERTY}=${TARGET_PROPERTY} ~trainer.logger data_module.properties=[\"${TARGET_PROPERTY}\"]"
    
    - name: "structure_generation"
      enabled: true
      commands:
        - "mkdir -p ${RESULTS_PATH}"
        - "mattergen-generate ${RESULTS_PATH} --pretrained-name=${MODEL_NAME} --batch_size=${BATCH_SIZE} --num_batches=${NUM_BATCHES} --properties_to_condition_on='{\"${TARGET_PROPERTY}\": ${TARGET_VALUE}}' --diffusion_guidance_factor=2.0"
    
    - name: "quality_evaluation"
      enabled: true
      commands:
        - "git lfs pull -I data-release/alex-mp/reference_MP2020correction.gz --exclude=''"
        - "mattergen-evaluate --structures_path=${RESULTS_PATH} --relax=True --save_as='${RESULTS_PATH}/metrics.json'"
    
    - name: "result_analysis"
      enabled: true
      commands:
        - "python scripts/quality_analysis.py --results_dir ${RESULTS_PATH}"
        - "python scripts/generate_report.py --metrics_file ${RESULTS_PATH}/metrics.json --output_file ${RESULTS_PATH}/report.html"
```

### æ‰¹é‡å®éªŒç®¡ç†

#### å‚æ•°æ‰«æè„šæœ¬
```python
# scripts/parameter_sweep.py
import itertools
import yaml
from pathlib import Path

class ParameterSweep:
    def __init__(self, base_config: str, sweep_config: str):
        self.base_config = yaml.load(Path(base_config).read_text())
        self.sweep_config = yaml.load(Path(sweep_config).read_text())
    
    def generate_experiments(self):
        """ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆçš„å®éªŒé…ç½®"""
        
        sweep_params = self.sweep_config['parameters']
        param_names = list(sweep_params.keys())
        param_values = list(sweep_params.values())
        
        experiments = []
        
        for i, combination in enumerate(itertools.product(*param_values)):
            # åˆ›å»ºå®éªŒé…ç½®
            exp_config = self.base_config.copy()
            exp_name = f"sweep_exp_{i:03d}"
            
            # åº”ç”¨å‚æ•°ç»„åˆ
            for param_name, param_value in zip(param_names, combination):
                self.set_nested_param(exp_config, param_name, param_value)
            
            # è®¾ç½®å®éªŒåç§°å’Œè¾“å‡ºç›®å½•
            exp_config['experiment']['name'] = exp_name
            exp_config['output_dir'] = f"experiments/{exp_name}"
            
            experiments.append((exp_name, exp_config))
        
        return experiments
    
    def set_nested_param(self, config: dict, param_path: str, value):
        """è®¾ç½®åµŒå¥—å‚æ•°å€¼"""
        keys = param_path.split('.')
        current = config
        
        for key in keys[:-1]:
            if key not in current:
                current[key] = {}
            current = current[key]
        
        current[keys[-1]] = value
    
    def run_sweep(self):
        """æ‰§è¡Œå‚æ•°æ‰«æ"""
        experiments = self.generate_experiments()
        
        for exp_name, exp_config in experiments:
            print(f"ğŸš€ æ‰§è¡Œå®éªŒ: {exp_name}")
            
            # ä¿å­˜å®éªŒé…ç½®
            exp_dir = Path(f"experiments/{exp_name}")
            exp_dir.mkdir(parents=True, exist_ok=True)
            
            config_file = exp_dir / "config.yaml"
            with open(config_file, 'w') as f:
                yaml.dump(exp_config, f, default_flow_style=False)
            
            # æ‰§è¡Œå®éªŒ
            workflow = AutomatedWorkflow(str(config_file))
            workflow.run_full_workflow()
            
            print(f"âœ… å®éªŒ {exp_name} å®Œæˆ")

# å‚æ•°æ‰«æé…ç½®ç¤ºä¾‹
# configs/sweep_config.yaml
"""
parameters:
  generation.guidance_factor: [1.0, 2.0, 3.0, 5.0]
  generation.batch_size: [16, 32, 64]
  model.target_property_value: [1.0, 1.5, 2.0]
"""
```

## ğŸ“š æœ€ä½³å®è·µ

### è®¡ç®—èµ„æºä¼˜åŒ–

#### GPU å†…å­˜ç®¡ç†
```python
# scripts/memory_optimizer.py
import torch
import psutil
from pathlib import Path

class MemoryOptimizer:
    def __init__(self):
        self.gpu_memory = self.get_gpu_memory()
        self.cpu_memory = self.get_cpu_memory()
    
    def get_gpu_memory(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            return torch.cuda.get_device_properties(0).total_memory / 1e9
        return 0
    
    def get_cpu_memory(self):
        """è·å–CPUå†…å­˜ä¿¡æ¯"""
        return psutil.virtual_memory().total / 1e9
    
    def recommend_batch_size(self, model_size: str = "base"):
        """æ¨èæ‰¹æ¬¡å¤§å°"""
        
        memory_requirements = {
            "base": {"gpu": 8, "cpu": 16},      # GB
            "large": {"gpu": 16, "cpu": 32},
            "xl": {"gpu": 32, "cpu": 64}
        }
        
        req = memory_requirements.get(model_size, memory_requirements["base"])
        
        if self.gpu_memory >= req["gpu"] and self.cpu_memory >= req["cpu"]:
            batch_sizes = {"base": 32, "large": 16, "xl": 8}
            return batch_sizes.get(model_size, 16)
        else:
            # å†…å­˜ä¸è¶³æ—¶çš„é™çº§å»ºè®®
            return max(1, int(self.gpu_memory / req["gpu"] * 16))
    
    def optimize_generation_config(self, target_structures: int):
        """ä¼˜åŒ–ç”Ÿæˆé…ç½®"""
        recommended_batch_size = self.recommend_batch_size()
        num_batches = target_structures // recommended_batch_size
        
        config = {
            "batch_size": recommended_batch_size,
            "num_batches": num_batches,
            "gradient_checkpointing": self.gpu_memory < 16,
            "mixed_precision": True,
            "num_workers": min(8, psutil.cpu_count())
        }
        
        return config
```

#### åˆ†å¸ƒå¼è®¡ç®—é…ç½®
```bash
# scripts/distributed_setup.sh
#!/bin/bash

# å¤šGPUè®­ç»ƒé…ç½®
export MASTER_ADDR=localhost
export MASTER_PORT=12355
export WORLD_SIZE=4  # GPUæ•°é‡

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=4 scripts/distributed_train.py \
  --config configs/distributed_config.yaml

# å¤šèŠ‚ç‚¹é…ç½® (å¦‚æœæœ‰å¤šå°æœºå™¨)
# ä¸»èŠ‚ç‚¹
torchrun --nnodes=2 --node_rank=0 --master_addr=MASTER_IP --master_port=12355 --nproc_per_node=4 scripts/distributed_train.py

# ä»èŠ‚ç‚¹
torchrun --nnodes=2 --node_rank=1 --master_addr=MASTER_IP --master_port=12355 --nproc_per_node=4 scripts/distributed_train.py
```

### è´¨é‡æ§åˆ¶æ£€æŸ¥ç‚¹

#### è´¨é‡æ£€æŸ¥è„šæœ¬
```python
# scripts/quality_checkpoints.py
class QualityCheckpoints:
    def __init__(self, config: dict):
        self.config = config
        self.checkpoints = [
            self.check_data_integrity,
            self.check_model_performance,
            self.check_generation_quality,
            self.check_evaluation_metrics
        ]
    
    def run_all_checks(self):
        """è¿è¡Œæ‰€æœ‰è´¨é‡æ£€æŸ¥"""
        results = {}
        
        for checkpoint in self.checkpoints:
            check_name = checkpoint.__name__
            try:
                result = checkpoint()
                results[check_name] = {"status": "pass", "details": result}
                print(f"âœ… {check_name}: PASS")
            except Exception as e:
                results[check_name] = {"status": "fail", "error": str(e)}
                print(f"âŒ {check_name}: FAIL - {e}")
        
        return results
    
    def check_data_integrity(self):
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        data_path = Path(self.config['data_path'])
        
        required_files = ['train/', 'val/']
        for file_path in required_files:
            if not (data_path / file_path).exists():
                raise FileNotFoundError(f"Missing required data: {file_path}")
        
        return {"data_files": "complete"}
    
    def check_model_performance(self):
        """æ£€æŸ¥æ¨¡å‹æ€§èƒ½"""
        # è¿è¡Œå°è§„æ¨¡æµ‹è¯•
        test_result = self.run_model_test()
        
        if test_result['loss'] > self.config['max_acceptable_loss']:
            raise ValueError(f"Model loss too high: {test_result['loss']}")
        
        return test_result
    
    def check_generation_quality(self):
        """æ£€æŸ¥ç”Ÿæˆè´¨é‡"""
        # ç”Ÿæˆå°æ‰¹é‡æ ·æœ¬è¿›è¡Œè´¨é‡æ£€æŸ¥
        test_structures = self.generate_test_structures()
        
        validity_rate = self.calculate_validity_rate(test_structures)
        if validity_rate < self.config['min_validity_rate']:
            raise ValueError(f"Validity rate too low: {validity_rate}")
        
        return {"validity_rate": validity_rate}
    
    def check_evaluation_metrics(self):
        """æ£€æŸ¥è¯„ä¼°æŒ‡æ ‡"""
        metrics = self.load_evaluation_metrics()
        
        for metric, threshold in self.config['metric_thresholds'].items():
            if metrics.get(metric, 0) < threshold:
                raise ValueError(f"Metric {metric} below threshold: {metrics[metric]} < {threshold}")
        
        return metrics
```

### é”™è¯¯å¤„ç†å’Œæ¢å¤

#### è‡ªåŠ¨é‡è¯•æœºåˆ¶
```python
# scripts/robust_executor.py
import time
import traceback
from functools import wraps

def retry_on_failure(max_retries=3, delay=60):
    """è£…é¥°å™¨ï¼šå¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"âŒ å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                    
                    if attempt < max_retries - 1:
                        print(f"â³ {delay}ç§’åé‡è¯•...")
                        time.sleep(delay)
                    else:
                        print("âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
                        traceback.print_exc()
                        raise
            
        return wrapper
    return decorator

@retry_on_failure(max_retries=3, delay=120)
def robust_generation(config):
    """å¸¦é‡è¯•æœºåˆ¶çš„ç¨³å¥ç”Ÿæˆ"""
    return run_generation(config)

@retry_on_failure(max_retries=2, delay=60)
def robust_evaluation(structures_path):
    """å¸¦é‡è¯•æœºåˆ¶çš„ç¨³å¥è¯„ä¼°"""
    return run_evaluation(structures_path)
```

#### æ£€æŸ¥ç‚¹æ¢å¤
```python
# scripts/checkpoint_manager.py
class CheckpointManager:
    def __init__(self, experiment_dir: str):
        self.experiment_dir = Path(experiment_dir)
        self.checkpoint_file = self.experiment_dir / "progress.json"
    
    def save_progress(self, stage: str, status: str, data: dict = None):
        """ä¿å­˜è¿›åº¦æ£€æŸ¥ç‚¹"""
        progress = self.load_progress()
        
        progress[stage] = {
            "status": status,
            "timestamp": datetime.now().isoformat(),
            "data": data or {}
        }
        
        with open(self.checkpoint_file, 'w') as f:
            json.dump(progress, f, indent=2)
    
    def load_progress(self):
        """åŠ è½½è¿›åº¦æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            return json.load(self.checkpoint_file.open())
        return {}
    
    def get_next_stage(self, stages: list):
        """è·å–ä¸‹ä¸€ä¸ªéœ€è¦æ‰§è¡Œçš„é˜¶æ®µ"""
        progress = self.load_progress()
        
        for stage in stages:
            if stage not in progress or progress[stage]["status"] != "completed":
                return stage
        
        return None  # æ‰€æœ‰é˜¶æ®µéƒ½å·²å®Œæˆ
    
    def resume_workflow(self, workflow_config):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤å·¥ä½œæµç¨‹"""
        stages = workflow_config['stages']
        next_stage = self.get_next_stage([s['name'] for s in stages])
        
        if next_stage:
            print(f"ğŸ”„ ä»é˜¶æ®µ '{next_stage}' æ¢å¤å·¥ä½œæµç¨‹")
            return next_stage
        else:
            print("âœ… æ‰€æœ‰é˜¶æ®µå·²å®Œæˆ")
            return None
```

---

## ğŸ¯ æ€»ç»“

### å·¥ä½œæµç¨‹ä¼˜åŠ¿
1. **æ ‡å‡†åŒ–**: æä¾›å¯é‡å¤çš„æ ‡å‡†æµç¨‹
2. **è‡ªåŠ¨åŒ–**: å‡å°‘äººå·¥å¹²é¢„å’Œé”™è¯¯
3. **å¯è¿½è¸ª**: å®Œæ•´çš„å®éªŒè®°å½•å’Œç‰ˆæœ¬æ§åˆ¶
4. **å¯æ‰©å±•**: æ”¯æŒä¸åŒåº”ç”¨åœºæ™¯çš„å®šåˆ¶

### å…³é”®æˆåŠŸè¦ç´ 
1. **å……åˆ†çš„å‰æœŸè§„åˆ’**: æ˜ç¡®ç›®æ ‡å’Œçº¦æŸæ¡ä»¶
2. **åˆé€‚çš„è®¡ç®—èµ„æº**: æ ¹æ®éœ€æ±‚é…ç½®ç¡¬ä»¶
3. **ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶**: åœ¨æ¯ä¸ªé˜¶æ®µè¿›è¡Œè´¨é‡æ£€æŸ¥
4. **å®Œå–„çš„æ–‡æ¡£è®°å½•**: ä¾¿äºç»“æœå¤ç°å’Œåˆ†æ

### æŒç»­æ”¹è¿›æ–¹å‘
1. **æµç¨‹è‡ªåŠ¨åŒ–**: è¿›ä¸€æ­¥å‡å°‘æ‰‹åŠ¨æ“ä½œ
2. **æ™ºèƒ½è°ƒä¼˜**: åŸºäºå†å²æ•°æ®è‡ªåŠ¨ä¼˜åŒ–å‚æ•°
3. **äº‘ç«¯éƒ¨ç½²**: æ”¯æŒå¤§è§„æ¨¡äº‘è®¡ç®—ç¯å¢ƒ
4. **ç¤¾åŒºé›†æˆ**: ä¸ææ–™æ•°æ®åº“å’Œå·¥å…·ç”Ÿæ€é›†æˆ

---

## ğŸ“ è·å–å¸®åŠ©

- **ä¸»æ–‡æ¡£**: [README_CN.md](README_CN.md)
- **å¾®è°ƒæŒ‡å—**: [README_FINETUNE_CN.md](README_FINETUNE_CN.md)
- **æ¶æ„æ–‡æ¡£**: [README_ARCHITECTURE_CN.md](README_ARCHITECTURE_CN.md)
- **GitHub Issues**: [https://github.com/microsoft/mattergen/issues](https://github.com/microsoft/mattergen/issues)

*æœ¬å·¥ä½œæµç¨‹æŒ‡å—åŸºäº MatterGen v1.0 ç¼–å†™*